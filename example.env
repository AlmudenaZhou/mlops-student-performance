# S3 Bucket to be used as artifact_path in mlflow
BUCKET_NAME=
# Name of the project. I have used student-performance.
EXPERIMENT_NAME=
# RunID of the production model chosen from the research
RUN_ID=
# folder where the artifacts has been saved by mlflow
ARTIFACT_FOLDER=
# folder where the models has been saved  by mlflow
MODEL_FOLDER=

# Fill this if you are testing S3 locally
S3_ENDPOINT_URL=

# mlflow host and port
MLFLOW_HOST=
MLFLOW_PORT=
# ExperimentID used to save all the mlflow experiments for this project
EXPERIMENT_ID=1
# Empty to download the artifacts without using the mlflow. If used locally: http://${MLFLOW_HOST}:${MLFLOW_PORT}
MLFLOW_TRACKING_URI=""
# URI of the backend (sqlite,localhost:5000,...)
MLFLOW_BACKEND_URI=
# URI where the artifacts has been saved. In my case, the S3 Bucket
MLFLOW_ARTIFACT_URI=

# Name of the kinesis stream for predictions
INPUT_STREAM_NAME=
PREDICTIONS_STREAM_NAME=
AWS_DEFAULT_REGION=
# Add this to run the model from local
MODEL_LOCATION=
# Add this to add kinesis from localstack
KINESIS_ENDPOINT_URL=

# Monitoring Postgres credentials
POSTGRES_HOST=
POSTGRES_PORT=
POSTGRES_DB_NAME=
POSTGRES_USER=
POSTGRES_PASSWORD=

# Monitoring alerts
# Email to verify in SES, alert sender.
SENDER_EMAIL=
# Emails list to sent the alerts to. Format: ["mail1@example.com", "mail2@example.com",...]
RECIPIENT_LIST=
# Link and project to be included in the email body
PROJECT_NAME=
LINK_URL=
